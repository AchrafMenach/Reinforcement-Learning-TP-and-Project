{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3691908-9c5e-4678-840b-e7257e1db59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "env = [\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, -10, 0, -10, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 10, 0, 0],\n",
    "    [0, 0, 0, 0, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff1604e-514a-4f6c-afd2-1db3f1651c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = len(env)\n",
    "cols = len(env[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c473a9fa-d4d3-47c9-8cfa-da3704c75527",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {}\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        rewards[(i, j)] = 0 \n",
    "\n",
    "def get_reward(state):\n",
    "    i, j = state\n",
    "    return env[i][j]\n",
    "visit_count = {(i, j): 0 for i in range(rows) for j in range(cols)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8db0c43-d973-40d3-b7b1-acda7eb0bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state, action):\n",
    "    i, j = state\n",
    "    if action == 'UP':\n",
    "        new_state = (max(i - 1, 0), j)\n",
    "    elif action == 'DOWN':\n",
    "        new_state = (min(i + 1, rows - 1), j)\n",
    "    elif action == 'LEFT':\n",
    "        new_state = (i, max(j - 1, 0))\n",
    "    elif action == 'RIGHT':\n",
    "        new_state = (i, min(j + 1, cols - 1))\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc5513e-d724-4ec9-a466-099d59428ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon=0.1):\n",
    "    possible_actions = ['UP', 'DOWN', 'LEFT', 'RIGHT'] # epsilon grand on explore , petit en exploite\n",
    "    if np.random.random() < epsilon:\n",
    "        # Exploration : choisir une action aléatoire\n",
    "        return np.random.choice(possible_actions)\n",
    "    else:\n",
    "        # Exploitation : choisir l'action avec la meilleure récompense attendue\n",
    "        best_action = None\n",
    "        best_reward = -float('inf')\n",
    "        for action in possible_actions:\n",
    "            new_state = take_action(state, action)\n",
    "            if rewards[new_state] > best_reward:\n",
    "                best_reward = rewards[new_state]\n",
    "                best_action = action\n",
    "        return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ae5fc5-8279-4c96-9570-fb077d39d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_reward(state, reward, alpha=0.1):\n",
    "    visit_count[state] += 1\n",
    "    rewards[state] += alpha * (reward - rewards[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1888a8b2-01fb-43b0-86c3-3a9fc159b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished with reward -10.\n",
      "Episode 1 finished with reward -10.\n",
      "Episode 2 finished with reward -10.\n",
      "Episode 3 finished with reward -10.\n",
      "Episode 4 finished with reward 10.\n",
      "Episode 5 finished with reward -10.\n",
      "Episode 6 finished with reward -10.\n",
      "Episode 7 finished with reward -10.\n",
      "Episode 8 finished with reward 10.\n",
      "Episode 9 finished with reward 10.\n",
      "Episode 10 finished with reward -10.\n",
      "Episode 11 finished with reward -10.\n",
      "Episode 12 finished with reward -10.\n",
      "Episode 13 finished with reward 10.\n",
      "Episode 14 finished with reward -10.\n",
      "Episode 15 finished with reward -10.\n",
      "Episode 16 finished with reward -10.\n",
      "Episode 17 finished with reward -10.\n",
      "Episode 18 finished with reward 10.\n",
      "Episode 19 finished with reward -10.\n",
      "Episode 20 finished with reward 10.\n",
      "Episode 21 finished with reward -10.\n",
      "Episode 22 finished with reward 10.\n",
      "Episode 23 finished with reward -10.\n",
      "Episode 24 finished with reward -10.\n",
      "Episode 25 finished with reward -10.\n",
      "Episode 26 finished with reward -10.\n",
      "Episode 27 finished with reward 10.\n",
      "Episode 28 finished with reward -10.\n",
      "Episode 29 finished with reward 10.\n",
      "Episode 30 finished with reward -10.\n",
      "Episode 31 finished with reward 10.\n",
      "Episode 32 finished with reward -10.\n",
      "Episode 33 finished with reward -10.\n",
      "Episode 34 finished with reward -10.\n",
      "Episode 35 finished with reward -10.\n",
      "Episode 36 finished with reward 10.\n",
      "Episode 37 finished with reward -10.\n",
      "Episode 38 finished with reward -10.\n",
      "Episode 39 finished with reward 10.\n",
      "Episode 40 finished with reward -10.\n",
      "Episode 41 finished with reward 10.\n",
      "Episode 42 finished with reward -10.\n",
      "Episode 43 finished with reward -10.\n",
      "Episode 44 finished with reward -10.\n",
      "Episode 45 finished with reward -10.\n",
      "Episode 46 finished with reward -10.\n",
      "Episode 47 finished with reward -10.\n",
      "Episode 48 finished with reward -10.\n",
      "Episode 49 finished with reward 10.\n",
      "Episode 50 finished with reward 10.\n",
      "Episode 51 finished with reward 10.\n",
      "Episode 52 finished with reward -10.\n",
      "Episode 53 finished with reward -10.\n",
      "Episode 54 finished with reward -10.\n",
      "Episode 55 finished with reward -10.\n",
      "Episode 56 finished with reward 10.\n",
      "Episode 57 finished with reward 10.\n",
      "Episode 58 finished with reward 10.\n",
      "Episode 59 finished with reward 10.\n",
      "Episode 60 finished with reward -10.\n",
      "Episode 61 finished with reward 10.\n",
      "Episode 62 finished with reward 10.\n",
      "Episode 63 finished with reward -10.\n",
      "Episode 64 finished with reward 10.\n",
      "Episode 65 finished with reward -10.\n",
      "Episode 66 finished with reward -10.\n",
      "Episode 67 finished with reward 10.\n",
      "Episode 68 finished with reward -10.\n",
      "Episode 69 finished with reward 10.\n",
      "Episode 70 finished with reward 10.\n",
      "Episode 71 finished with reward -10.\n",
      "Episode 72 finished with reward -10.\n",
      "Episode 73 finished with reward -10.\n",
      "Episode 74 finished with reward -10.\n",
      "Episode 75 finished with reward -10.\n",
      "Episode 76 finished with reward 10.\n",
      "Episode 77 finished with reward -10.\n",
      "Episode 78 finished with reward 10.\n",
      "Episode 79 finished with reward 10.\n",
      "Episode 80 finished with reward -10.\n",
      "Episode 81 finished with reward -10.\n",
      "Episode 82 finished with reward 10.\n",
      "Episode 83 finished with reward 10.\n",
      "Episode 84 finished with reward 10.\n",
      "Episode 85 finished with reward -10.\n",
      "Episode 86 finished with reward -10.\n",
      "Episode 87 finished with reward 10.\n",
      "Episode 88 finished with reward -10.\n",
      "Episode 89 finished with reward -10.\n",
      "Episode 90 finished with reward 10.\n",
      "Episode 91 finished with reward -10.\n",
      "Episode 92 finished with reward -10.\n",
      "Episode 93 finished with reward -10.\n",
      "Episode 94 finished with reward -10.\n",
      "Episode 95 finished with reward 10.\n",
      "Episode 96 finished with reward -10.\n",
      "Episode 97 finished with reward 10.\n",
      "Episode 98 finished with reward 10.\n",
      "Episode 99 finished with reward 10.\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "max_steps_per_episode = 50\n",
    "epsilon = 1.0  # Taux d'exploration initial\n",
    "epsilon_decay = 0.99  # Décroissance de epsilon\n",
    "min_epsilon = 0.01  # Taux d'exploration minimal\n",
    "# Apprentissage\n",
    "for episode in range(episodes):\n",
    "    state = (0, 0)  # État initial\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        if steps >= max_steps_per_episode:\n",
    "            print(f\"Episode {episode} stopped: too many steps.\")\n",
    "            break\n",
    "\n",
    "        action = choose_action(state, epsilon)\n",
    "        new_state = take_action(state, action)\n",
    "        reward = get_reward(new_state)\n",
    "\n",
    "        if reward == 0:\n",
    "            reward = -1  # Pénalité pour les états neutres\n",
    "\n",
    "        update_reward(new_state, reward)\n",
    "        visit_count[new_state] += 1\n",
    "\n",
    "        state = new_state\n",
    "        steps += 1\n",
    "\n",
    "        if env[state[0]][state[1]] in [-10, 10]:\n",
    "            print(f\"Episode {episode} finished with reward {reward}.\")\n",
    "            break\n",
    "\n",
    "    # Décroissance de epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay, min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d9cf926-c00a-4642-9d4d-9a82eb8b6faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Récompenses moyennes pour chaque état :\n",
      "État (0, 0) : -1.00\n",
      "État (0, 1) : -1.00\n",
      "État (0, 2) : -0.99\n",
      "État (0, 3) : -0.94\n",
      "État (0, 4) : -0.94\n",
      "État (1, 0) : -1.00\n",
      "État (1, 1) : -9.94\n",
      "État (1, 2) : -0.94\n",
      "État (1, 3) : -7.71\n",
      "État (1, 4) : -0.75\n",
      "État (2, 0) : -1.00\n",
      "État (2, 1) : -0.97\n",
      "État (2, 2) : -0.92\n",
      "État (2, 3) : -0.52\n",
      "État (2, 4) : -0.65\n",
      "État (3, 0) : -0.97\n",
      "État (3, 1) : -0.90\n",
      "État (3, 2) : 9.82\n",
      "État (3, 3) : -0.57\n",
      "État (3, 4) : -0.65\n",
      "État (4, 0) : -0.89\n",
      "État (4, 1) : -0.88\n",
      "État (4, 2) : -0.69\n",
      "État (4, 3) : -0.52\n",
      "État (4, 4) : -0.57\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRécompenses moyennes pour chaque état :\")\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        val = rewards[(i, j)]\n",
    "        if visit_count[(i, j)] == 0:\n",
    "            print(f\"État ({i}, {j}) : Jamais visité\")\n",
    "        else:\n",
    "            print(f\"État ({i}, {j}) : {val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67a262-8c78-4505-8579-13f60f631ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploration finale :\n",
      "Step 0: (0, 0)\n",
      "Step 1: (1, 0)\n",
      "Step 2: (2, 0)\n",
      "Step 3: (3, 0)\n",
      "Step 4: (4, 0)\n",
      "Step 5: (4, 1)\n",
      "Step 6: (4, 2)\n",
      "Nee9iii!\n"
     ]
    }
   ],
   "source": [
    "#Exploration finale avec la politique apprise\n",
    "state = (0, 0)\n",
    "steps = 0\n",
    "visited_states = set()\n",
    "\n",
    "print(\"\\nExploration finale :\")\n",
    "while True:\n",
    "    print(f\"Step {steps}: {state}\")\n",
    "\n",
    "    if state in visited_states:\n",
    "        print(\"Boucle infini.\")\n",
    "        break\n",
    "    visited_states.add(state)\n",
    "\n",
    "    # Choisir la meilleure action basée sur les récompenses\n",
    "    possible_actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    best_action = None\n",
    "    best_reward = -float('inf')\n",
    "\n",
    "    for action in possible_actions:\n",
    "        new_state = take_action(state, action)\n",
    "        if rewards[new_state] > best_reward:\n",
    "            best_reward = rewards[new_state]\n",
    "            best_action = action\n",
    "\n",
    "    state = take_action(state, best_action)\n",
    "    steps += 1\n",
    "\n",
    "    if env[state[0]][state[1]] == -10:\n",
    "        print(\"Loser.\")\n",
    "        break\n",
    "    elif env[state[0]][state[1]] == 10:\n",
    "        print(\"Winner!\")\n",
    "        break\n",
    "\n",
    "    if steps > 50:\n",
    "        print(\"Boucle infini.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f89d8-5c49-46ed-8c77-d2e0a513e777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731fd39-d1e4-43b3-a445-92609bdf7624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320a6bd-5e66-47c5-85e3-7a131a43c36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
