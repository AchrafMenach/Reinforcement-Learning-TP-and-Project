# ğŸ† Apprentissage par Renforcement - Taxi-v3

![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)  
![Gymnasium](https://img.shields.io/badge/Gymnasium-0.29%2B-orange.svg)  
ImplÃ©mentation d'un algorithme PPO pour rÃ©soudre le problÃ¨me Taxi-v3 de Gymnasium.

## ğŸ“– Introduction

Ce projet dÃ©montre l'application de l'algorithme PPO (Proximal Policy Optimization) sur l'environnement Taxi-v3, oÃ¹ un agent taxi doit apprendre Ã  prendre et dÃ©poser des passagers de maniÃ¨re optimale.

## ğŸš€ FonctionnalitÃ©s ClÃ©s

- ğŸ”¹ **Environnement Taxi-v3** avec 500 Ã©tats et 6 actions
- ğŸ¯ **Algorithme PPO** avec clipping des politiques
- ğŸ“ˆ **Reward Shaping** personnalisÃ© pour accÃ©lÃ©rer l'apprentissage
- ğŸ“Š **Monitoring avancÃ©** des performances
- ğŸ **Early Stopping** lorsque le taux de succÃ¨s atteint 90%

---

## âš™ï¸ Installation et ExÃ©cution

### 1ï¸âƒ£ PrÃ©requis  
Python 3.8+ et pip installÃ©s

### 2ï¸âƒ£ Installer les dÃ©pendances  
```sh
pip install gymnasium numpy tqdm matplotlib seaborn
```

### 3ï¸âƒ£ ExÃ©cuter l'entraÃ®nement et Visualiser les rÃ©sultats
```sh
python TP3.py
```


---

## ğŸ› ï¸ ParamÃ¨tres OptimisÃ©s

```python
gamma = 0.99               # Facteur d'actualisation
lr_policy = 0.001          # Taux d'apprentissage politique
lr_value = 0.1             # Taux d'apprentissage valeur
clip_epsilon = 0.2         # ParamÃ¨tre de clipping PPO
num_train_episodes = 20000 # Nombre maximal d'Ã©pisodes
entropy_coeff = 0.01       # Coefficient d'entropie
```

---

## ğŸ“Š RÃ©sultats

| MÃ©trique               | Avant EntraÃ®nement | AprÃ¨s EntraÃ®nement |
|------------------------|--------------------|--------------------|
| RÃ©compense moyenne     | -763.55 Â± 112.88   | -329.28 Â± 730.34   |
| Taux de succÃ¨s         | 0%                 | 76%                |
| Longueur moyenne       | -                  | 49.5               |
| Ã‰pisodes nÃ©cessaires   | -                  | 12,500             |

---

## ğŸ® Fonctionnement de l'Agent

1. **Initialisation** des tables de politique et de valeur
2. **Collecte** des trajectoires d'Ã©pisodes
3. **Calcul** des rÃ©compenses actualisÃ©es et avantages
4. **Mise Ã  jour** des politiques avec clipping PPO
5. **Optimisation** de la fonction de valeur
6. **Ã‰valuation** pÃ©riodique des performances

---

## ğŸ“œ Licence & Auteur

ğŸ“Œ **Auteur**: [Achraf Menach]  
ğŸ“… **Date**: Juin 2024  
ğŸ“œ **Licence**: MIT  
ğŸ’¡ **Contact**: [menachachraf3@gmail.com]  

> ğŸ’¡ **Astuce**: Activez `render_mode="human"` pour visualiser l'agent en action!


