{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937eeb49-c153-41c8-a3eb-57200dd309a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a5ac81-db40-4fc3-b4b8-ac72b5b3ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c6d188-1435-484f-b82a-9d40630beab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size=env.action_space.n\n",
    "state_space_size=env.observation_space.n\n",
    "q_table=np.zeros((state_space_size,action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d966f5c2-a333-43d5-8fbb-150a9a81fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=10000 #that we want an agent to play during the training \n",
    "max_steps_per_episode=100 # max number of stept that the agent take in one single episodes\n",
    "\n",
    "learning_rate=0.1 \n",
    "discount_rate=0.99 \n",
    "\n",
    "exploration_rate=1 #epsilon gredy \n",
    "max_exploration_rate=1\n",
    "min_exploration_rate=0.01\n",
    "exploration_decay_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c0c2983-b94c-4b40-8052-8400958ec50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average reward per thousand episodes \n",
      "\n",
      "1000 :  0.291\n",
      "2000 :  0.725\n",
      "3000 :  0.886\n",
      "4000 :  0.961\n",
      "5000 :  0.971\n",
      "6000 :  0.986\n",
      "7000 :  0.991\n",
      "8000 :  0.986\n",
      "9000 :  0.995\n",
      "10000 :  0.99\n",
      "\n",
      "\n",
      "******** Q-table ********\n",
      "\n",
      "[[0.94148015 0.95099005 0.95099005 0.94148015]\n",
      " [0.94148015 0.         0.96059601 0.95099005]\n",
      " [0.95099005 0.970299   0.95099005 0.96059601]\n",
      " [0.96059601 0.         0.95099005 0.95099005]\n",
      " [0.95099005 0.96059601 0.         0.94148015]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.         0.96059601]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.96059601 0.         0.970299   0.95099005]\n",
      " [0.96059601 0.9801     0.9801     0.        ]\n",
      " [0.970299   0.99       0.         0.970299  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.99       0.970299  ]\n",
      " [0.9801     0.99       1.         0.9801    ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "reward_all_episodes = []\n",
    "\n",
    "# Q-learning\n",
    "for episode in range(num_episodes):  \n",
    "    state = env.reset()[0]  \n",
    "    done = False\n",
    "    reward_current_episode = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):  \n",
    "        # Exploration-Exploitation \n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state, :])  # Exploitation\n",
    "        else:\n",
    "            action = env.action_space.sample()  # Exploration\n",
    "\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated  \n",
    "\n",
    "        # Update the Q-table\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "                                 learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        reward_current_episode += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # minimise exploration rate\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "\n",
    "    reward_all_episodes.append(reward_current_episode)\n",
    "\n",
    "\n",
    "rewards_per_thousand_episodes = np.array_split(np.array(reward_all_episodes), num_episodes // 1000)\n",
    "count = 1000\n",
    "print(\" Average reward per thousand episodes \\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r) / 1000))\n",
    "    count += 1000\n",
    "\n",
    "print(\"\\n\\n******** Q-table ********\\n\") \n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e952123a-ea92-4d2f-9158-a01c56919864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You reached the goal! \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"\\n\\n Testing the trained agent \\n\")\n",
    "for episode in range(5):  \n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    print(\"Episode\", episode + 1)\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        action = np.argmax(q_table[state, :])  \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        terminated = bool(terminated)\n",
    "        truncated = bool(truncated)    \n",
    "        \n",
    "        done = terminated or truncated  \n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\" You winl  \")\n",
    "            else:\n",
    "                print(\" You Lose \")\n",
    "            time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0fe0b6-559d-4062-8f06-aa1b26ead4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
